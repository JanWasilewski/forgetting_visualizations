{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jw7630/repos/LVEBM4CL/.venv/lib/python3.10/site-packages/torch/nn/modules/lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training without Experience Replay\n",
      "Training on Task 1\n",
      "Training on Task 2\n",
      "Training on Task 3\n",
      "Training on Task 4\n",
      "Training on Task 5\n",
      "\n",
      "Training with Experience Replay\n",
      "Training on Task 1\n",
      "Training on Task 2\n",
      "Training on Task 3\n",
      "Training on Task 4\n",
      "Training on Task 5\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from utils import *\n",
    "from models import *\n",
    "\n",
    "# Load Data\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "mnist_train = datasets.CIFAR10(root=\"./data\", train=True, transform=transform, download=True)\n",
    "mnist_test = datasets.CIFAR10(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "# Split dataset into 5 tasks with 2 classes each\n",
    "tasks = split_mnist_by_classes(mnist_train)\n",
    "tasks_test = split_mnist_by_classes(mnist_train)\n",
    "\n",
    "# Create data loaders for each task\n",
    "batch_size = 64\n",
    "data_loaders = [DataLoader(task, batch_size=batch_size, shuffle=True) for task in tasks]\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train without experience replay\n",
    "model = SimpleCNN()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "print(\"\\nTraining without Experience Replay\")\n",
    "train_model(model, data_loaders, tasks_test, optimizer, criterion, path=\"CIFAR10\", animation=False)\n",
    "\n",
    "# Train with experience replay\n",
    "replay_model = SimpleCNN()\n",
    "replay_optimizer = optim.Adam(replay_model.parameters(), lr=0.001)\n",
    "replay_buffer = ReplayBufferReservoir(capacity=100)\n",
    "print(\"\\nTraining with Experience Replay\")\n",
    "train_model(replay_model, data_loaders, tasks_test, replay_optimizer,\n",
    "             criterion, replay_buffer=replay_buffer, path=\"CIFAR10_ER\", animation=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
